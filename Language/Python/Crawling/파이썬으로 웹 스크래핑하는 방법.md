# 파이썬으로 웹 스크래핑하는 방법

출처: https://blog.advenoh.pe.kr/python/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9C%BC%EB%A1%9C-%EC%9B%B9-%EC%8A%A4%ED%81%AC%EB%9E%98%ED%95%91%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95-Web-scraping/



# 1. 소개

웹 정보 바다라고 할 만큼 엄청난 양의 데이터를 가지고 있습니다. 트위터, 페이스북과 같은 사이트에서는 정규화된 JSON 형태의 데이터를 API로 제공해서 쉽게 원하는 데이터에 접근할 수 있습니다. 하지만, API를 통해서 제공되는 데이터는 제한적이고 원하는 데이터를 얻지 못할 수도 있습니다.

필요한 정보를 실제 사이트에서 직접 데이터를 추출해서 데이터를 가공할 필요가 있습니다. 이런 방식을 웹 크롤링(Web Crawling), 웹 스크래핑(Web Scraping)이라고 합니다. 웹 크롤링은 웹 스파이터(spider), 봇(bot)이라고 해서 검색 엔진과 같은 여러 사이트(ex. 구글)에서 정보를 정기적으로 추출하는 방식입니다. 웹 스크래핑은 웹 사이트로에서 원하는 데이터를 추출하는 행위를 일반적으로 얘기합니다. 둘의 차이점을 정리하면 아래와 같습니다.

- 웹 크롤링
  - 검색 엔진에서 사용되며 bot과 같이 자동으로 웹 처리됨
  - 다운로드한 사이트를 index하여 사용자가 빠르게 원하는 것을 검색할 수 있도록 해줌
- 웹 스크래핑
  - 웹 사이트에서 원하는 데이터를 추출함
  - 추출한 데이터를 원하는 형식으로 가공함

# 2. 웹 스크래핑

파이썬이 웹 스크래핑을 하는 데 가장 많이 사용됩니다. Nodejs에서도 [Cheerio 모듈](https://medium.com/@moralmk/web-scraping-with-node-js-9a289ad19558) 을 사용해서 쉽게 원하는 데이터를 추출할 수 있지만, 본 포스트에서는 파이썬으로 웹 스크래핑하는 방법을 알아봅니다.

웹 스크래핑을 할 때는 3가지 정도의 단계를 거치게 됩니다.

1. Scraping - 데이터 가져오기
2. Parsing - 데이터 파씽
3. Manipulation - 데이터 가공

먼저 필요한 파이썬 모듈을 설치하고 각 모듈의 사용법을 알아봅시다.

## 2.1 필요한 패키지 설치 및 사용방법

설치 방법은 맥 OS 기반으로 작성되었습니다.

- Beautiful Soup
  - HTML과 XML 형식의 데이터를 보다 쉽게 파씽하고 다루는 모듈
  - 현재 버전은 bs4
- urllib
  - URL를 다루는 모듈
  - 파이썬에 기본적으로 내장되어 있는 모듈임
- requests
  - HTTP/1.1 요청를 보낼 수 있음
  - 요청 내용에 헤더, 폼 데이터, multipart 파일과 parameter를 포함해서 보낼 수 있음

### 2.1.1 패키지 설치

파이썬 패키지 관리자 명령어(pip)로 필요한 패키지를 설치합니다.

```bash
$ pip3 install beautifulsoup4
$ pip3 install requests
```

### 2.1.2 사용법과 예제

먼저 파이썬에 기본적으로 내장된 urllib 모듈을 사용해서 데이터를 가져와 보고 이어서 requests 모듈로 데이터를 가져오는 예제를 작성해봅니다. 전체 예제 소스는 [github](https://github.com/kenshin579/tutorials-python/tree/master/web_scraping/01_web_scraping) 에 작정 되어 있습니다. 위키백과 사이트에서 **요즘 화제** 페이지의 **주요 뉴스** 정보를 가져오는 예제를 같이 작성해봅시다.

- https://ko.wikipedia.org/wiki/포털:요즘_화제

![wiki](.\images\wiki.png)tps://blog.advenoh.pe.kr/static/e8dec998c76c6dc7bdf019d1b3a37f60/19ab6/wiki.png)

**1. 크롬의 개발자 도구를 열어 원하는 부분의 태그를 확인합니다.**

[![6B835F07 A3A4 4479 88DF 6F3B0F8D66D7](C:\GitHub\computer_note\Language\Python\Crawling\images\6B835F07-A3A4-4479-88DF-6F3B0F8D66D7.png)](https://blog.advenoh.pe.kr/static/05c53a10c8d90e3086bfcb614fc60a81/b2c3d/6B835F07-A3A4-4479-88DF-6F3B0F8D66D7.png)

**2. 웹사이트에 접근하여** **BeautifulSoup** **로 HTML를 파씽하고 원하는 데이터를 추출합니다.**

아래 코드는 urllib 모듈로 위키백과 사이트에 접근하는 방식입니다.

```python
from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen(WIKI_URL)  urllib로 html 가져오기
bsObj = BeautifulSoup(html, "html.parser")
main_news = bsObj.find("table", {"class": "vevent”})  크롬에서 확인한 부분으로 작성함
```

다음은 urllib 모듈 대신 requests 모듈을 사용하여 html을 가져오는 방식입니다. 스크립트를 작성하면 어쩔 수 없이 해당 웹 사이트에 자주 접속할 수 밖에 없는데, urlib 모듈로 접근하면 서버 로그에 urllib로 접속한다는 정보가 고스란히 남게 되고 또한 자주 접근하는 패턴으로 차단될 리스크가 있습니다. 하지만, requests 모듈은 headers에 추가 정보를 담아서 보낼 수 있어서 크롬이나 파이어폭스 브라우저가 보내는 정보를 담아서 보낼 수 있어 차단될 가능성이 적어 requests 모듈을 사용할 것을 추천합니다. 블랙 리스트에 등록되는 것을 피하는 여러 방법은 다음 포스트에 구체적으로 다루도록 하겠습니다.

```python
import requests
from bs4 import BeautifulSoup

session = requests.Session()
headers = {
"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome",
"Accept": "text_html,application_xhtml+xml,application_xml;q=0.9,image_webp,**/**;q=0.8",
"Connection": "close"
}

bsObj = BeautifulSoup(session.get(WIKI_URL, headers=headers).content, "html.parser”) requests로 url에 접근해 html를 가져옴
main_news = bsObj.find("table", {"class": "vevent"})
```

HTML을 가져온 다음 BeautifulSoup에서 어떻게 파씽을 하고 원하는 데이터를 추출하는지 아래 코드를 보면서 알아봅시다.

```python
bsObj = BeautifulSoup(html, "html.parser")
main_news = bsObj.find("table", {"class": "vevent"})
tr_all = main_news.find("table").find_all("tr")

 # title
print(tr_all[0].get_text().strip())

 # ui list
news_all = tr_all[1].find_all("li")

for each_tr in news_all:
text = each_tr.get_text().strip().replace("\n", " ")
striped_text = re.sub('\s\s+', " ", text)
print(striped_text)
```

라인 1에서는 파이썬에 기본으로 들어 있는 html.parser로 HTML를 파씽합니다. lxml등과 같은 외부 파써도 사용할 수도 있습니다. (pip로 설치 필요) ex. bsObj = BeautifulSoup(html, ‘lxml’)

라인 2부터는 주요 뉴스를 포함하고 있는 vevent 클래스의 내용 전체를 먼저 얻어오고 한 번 더 tr 부분을 추출해 옵니다.

tr [0] - 메인 타이틀 tr [1] - 뉴스 내용

얻어온 태그 내용의 텍스트 부분을 추출하려면 get_text() 함수를 이용하고 불필요한 whitespace는 strip()나 replace()함수로 제거합니다. 실행 결과는 다음과 같습니다.

[![01 execution](C:\GitHub\computer_note\Language\Python\Crawling\images\01_execution.png)](https://blog.advenoh.pe.kr/static/fed0e3659990db113cb38d4e23e9b960/19ab6/01_execution.png)

# 3. 추가 예제

인터넷상에서 많은 데이터가 존재하기 때문에 웹 스크래핑 기술로 다양한 데이터를 만들어 낼 수 있습니다.

- 같은 한 제품의 가격에 대해 비교 할 수 있도록 스크래핑 (ex. 다나와)
- 여러 소셜 네트워크(ex. 트위터)에서 회사의 제품에 대한 feedback을 얻을 수 있도록 스크래핑

저는 개인적으로 리디북스 페이퍼(이북 리더기)에서 성경을 종종 읽고 싶어서 EPUB를 만들면 좋겠다고 생각을 했었습니다. 웹 스크래핑 기술을 익히고 나서 EPUB 포맷으로 변환하는 스크립트를 작성했습니다. 아이디어는 위 예제와는 크게 차이는 없습니다.

https://github.com/kenshin579/app-korean-catholic-bible

# 4. 정리

웹 스크래핑을 하다 보면 접속하는 사이트로부터 차단될 수 있어서, 어떻게 하면 차단 당하지 않고 웹 스크래핑할 수 있는지 다음 포스트([웹 스크래핑하면서 차단 방지하는 방법](http://advenoh.tistory.com/3))에서 알아보도록 하죠.

# 5. 참고

조금 더 아래 책을 추천드립니다.

- 책 : [Web Scraping with Python](http://www.hanbit.co.kr/store/books/look.php?p_code=B7159663510)
  - [![image 1](C:\GitHub\computer_note\Language\Python\Crawling\images\image_1.jpg)](https://blog.advenoh.pe.kr/static/69c6891c2f4747ae2dbdf6b2a0265d7f/066f9/image_1.jpg)
- 스콜링 vs. 스크래핑
  - http://stophyun.tistory.com/142
  - https://ko.wikipedia.org/wiki/웹_크롤러
- Nodejs로 웹 스크래핑
  - https://medium.com/@moralmk/web-scraping-with-node-js-9a289ad19558





# 웹 스크래핑하면서 차단 방지하는 방법

# 1. 소개

스크래핑하면 사이트에 접속하여 데이터를 추출해야 해서 어떻게 작성하느냐에 따라 서버에 많은 부하를 줄 수도 있게 됩니다. 웹 서버를 담당하는 측에서는 서버에 많은 부하를 줄이기 위해 악의적으로? 접속하는 곳을 차단할 수밖에 없습니다. 이번 포스트에서는 웹 스크래핑을 하면서 사이트로부터 차단되지 않는 여러 방법에 대해서 알아보도록 하죠.

- robots.txt 체크하기
- User Agents 설정하기
- 잠시 sleep해서 부하 줄이기
- IP rotation - Tor

# 2. 웹 스크래핑시 차단 되지 않는 여러 방법

## 2.1 robots.txt 체크하기

robots.txt 파일은 웹 크롤러(검색봇)가 크롤링을 할 수 있고 없고 하는 웹 페이지를 정의한 파일입니다. 많은 웹 사이트(ex. [구글](http://www.google.com/robots.txt), [네이버](http://www.naver.com/robots.txt))에서 robots.txt 파일을 정의해두고 있습니다.

```text
# Example
User-agent: *
Allow: /
Disallow: /search.php
```

- User-agent : 대상 크롤러 (*: 모든 검색봇, googlebot: 구글봇 등등)
- Allow : 허용하는 경로
- Disallow : 허용하지 않는 경로

robots.txt 파일이 존재한다면, 접근하지 말아야 하는 경로가 무엇인지 미리 파악하여 웹 스크래핑시 해당 경로는 접근하지 않도록 주의를 해야 합니다.

## 2.2 User Agents 설정하기

아래 사이트는 접속하는 브라우져 속성이 웹 서버에 어떻게 보여지는 지 잘 테스트할 수 있는 사이트입니다.

- [http://www.whatismybrowser.com](http://www.whatismybrowser.com/)

제 컴퓨터에서 크롬 브라우져로 위 사이트에 접속을 해보면, 맥 크롬 68 버전임을 알수 있습니다.

[![image 2](C:\GitHub\computer_note\Language\Python\Crawling\images\image_2.png)](https://blog.advenoh.pe.kr/static/31bcbddcd0bf4bca9eb5beab31cb134a/0fb99/image_2.png)

웹 사이트에 접속할때 서버로 보내는 HTTP 헤더를 확인해보면 다양한 정보를 담아서 보냅니다. 특히 User-Agent의 정보는 현재 사용하는 브라우져를 알수 있습니다.

[![image 8](C:\GitHub\computer_note\Language\Python\Crawling\images\image_8.png)](https://blog.advenoh.pe.kr/static/5060a0d8dc342c931e3e7695f64b83e1/41d3b/image_8.png)

urllib 모듈로 사이트에 접속하면 HTTP 헤더 정보에는 python-urllib로 접속했음을 알수 있고 이 정보로 서버 측 담당자들은 쉽게 해당 접속이 일반 사용자가 아님을 판단할 수 있으므로 차단을 할 확률이 높습니다. 되도록이면 웹 스크래핑할 때는 urllib모듈은 사용하지 말고 requests 모듈을 사용하여 헤더 정보를 수정해 보내는게 좋습니다.

[![image 5](C:\GitHub\computer_note\Language\Python\Crawling\images\image_5.png)](https://blog.advenoh.pe.kr/static/defbba17af77c526a16e035b0475d9de/0fb99/image_5.png)

아래는 requests 모듈로 header 정보에 크롬 브라우져의 User-Agent를 만들어서 보내는 방법입니다.

```python
import requests

session = requests.Session()
headers = {
"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome",
"Accept": "text_html,application_xhtml+xml,application_xml;q=0.9,image_webp,**/**;q=0.8"
}
html = session.get(WIKI_URL, headers=headers).content
bsObj = BeautifulSoup(html, "html.parser”)
```

## 2.3 잠시 sleep해서 부하 줄이기

사용자가 실제 사이트에 접속해서 활동하는 것보다 더 빠르게 여러 페이지에 접속하고 온라인 폼을 채워서 스크래핑 한다면 일단 사용자가 아니라는 인식을 주게 되어 차단될 수 있습니다. 또한, 반복문으로 여러 페이지를 로딩하여 처리하거나 멀티 쓰레드 프로그래밍 방식으로 처리하면 서버에 부하를 많이 줄 수 있게 됩니다. 각 페이지에 접속하고 데이터 요청을 하는 건 최소한으로 하는게 좋습니다. 그러기 위해 sleep 문으로 각 페이지에 접속 할때는 조금의 간격을 두고 접속하면 부하를 줄일 수 있습니다.

```python
//사용자가 접속하는 것처럼 램덤 값 사용

rand_value = randint(1, MAX_SLEEP_TIME)
time.sleep(rand_value)
```

## 2.4 IP rotation - Tor

토르(Tor)란 The Onion Router의 약칭 트래픽 분석이나 IP 주소 추적을 불가능케 하는 암호화 라우터 네트워크입니다. 전송되는 데이터는 토르 네트워크를 라우팅할 때 마다 모든 경로에서 암호화가 되어 중간에 패킷을 얻어 풀려고 시도해도 실제 IP 소스 주소를 알아내기란 쉽지 않습니다. 거의 불가능? 하다고 설명하고 있습니다. (책: 파이썬으로 웹 스크래핑)

토르의 실제 동작 원리는 링크(참고 #4.1)를 참고해주세요. 여기서는 실제 어떻게 토르 네트워크안에서 웹 스크래핑을 할 수 있는지 다룹니다.

토르 설치 및 실행 내용은 맥 기반으로 작성 되었습니다. 다른 OS 설치는 아래 링크(참고 #4.5)를 참조하세요.

### 2.4.1 Tor 설치

```bash
$ brew install tor
$ tor
```

tor 명령어로 토르는 실행할 수 있지만, 네트워크 트래픽은 아직 토르를 통해서 전송되지 않습니다.

### 2.4.2 Tor 설정 및 실행

모든 시스템 트래픽이 토르를 통해서 라우팅 되려면 시스템의 네트워크 세팅을 변경해줘야 합니다. 사용하는(ex. Wi-Fi, Ethernet) 네트워크마다 세팅을 했다가 기본 네트워크를 사용하려면 다시 세팅을 원복해야 하는 번거로움 있습니다. 네트워크 세팅을 변경해주는 부분을 쉽게 bash 스크립트로 짤 수 있습니다(kremalicious 웹사이트 참고 #4.2).

아래는 tor.sh 스크립트를 실행한 화면입니다.

```bash
$ tor.sh
```

[![image 1](C:\GitHub\computer_note\Language\Python\Crawling\images\image_1.png)](https://blog.advenoh.pe.kr/static/934a5e5e08ceef1688f11e66104e01a5/4ff83/image_1.png)

토르 세팅이 잘 되었는지 확인하려면 아래 사이트에 접속하여 확인할 수 있습니다.

- https://check.torproject.org/ | 일반 네트워크로 접속 | 토르 네트워크로 접속 | | -------------------- | -------------------- | |![C16084A1 73EC 4580 83F0 F5D482F30793](C:\GitHub\computer_note\Language\Python\Crawling\images\C16084A1-73EC-4580-83F0-F5D482F30793.png)|![AFB09BBA 2A47 4B2F A6C3 2D4DCE4EC142](C:\GitHub\computer_note\Language\Python\Crawling\images\AFB09BBA-2A47-4B2F-A6C3-2D4DCE4EC142.png)|

현재 사용 중인 공개 IP 주소와 Tor IP 주소 또한 명령어로 확인할 수 있습니다. 토르 네트워크 확인을 위해서는 torsocks 패키지를 설치해야 합니다.

```bash
$ brew install torsocks
$ torsocks wget -q0- http://icanhazip.com/; echo
```

[![3CA8367A 1885 43C8 AF38 98D920F3CDD5](C:\GitHub\computer_note\Language\Python\Crawling\images\3CA8367A-1885-43C8-AF38-98D920F3CDD5.png)](https://blog.advenoh.pe.kr/static/d79c635d90a70a4882b3451d71466c73/4ff83/3CA8367A-1885-43C8-AF38-98D920F3CDD5.png)

### 2.4.3 토르 네트워크에서 웹 스크래핑하기

파이썬에서 토르 네트워크로 웹 스크래핑을 하려면 SOCKS proxy client 모듈이 필요합니다. pip 명령어로 설치합니다.

```bash
$ pip install pysocks
#!/usr/bin/env python3
import sys

import requests

def main():
url = "http://icanhazip.com/"
proxies = {
'http': 'socks5://127.0.0.1:9050',
'https': 'socks5://127.0.0.1:9050'
}

response = requests.get(url, proxies=proxies)
print('tor ip: {}'.format(response.text.strip()))

if __name__ == "__main__":
sys.exit(main())
```

torsocks 명령어로 얻은 IP 주소와 위 파이썬 스크립트로 실행한 주소가 같음을 확인할 수 있습니다.

[![F088A986 3EE3 4F16 BE74 4050809E949A](C:\GitHub\computer_note\Language\Python\Crawling\images\F088A986-3EE3-4F16-BE74-4050809E949A.png)](https://blog.advenoh.pe.kr/static/6e3a0296ebaa73e0f73d99ea11dfa07f/561da/F088A986-3EE3-4F16-BE74-4050809E949A.png)

# 3. 웹 스크래핑 작성시 고려사항

## 3.1 코드 작성시 unit test으로 작성하기

스크립트를 작성할때 매번 웹 사이트에 접근하여 코드를 작성해야 합니다. 매번 사이트에 접속하기 보다는 필요한 HTML 부분을 크룸 브라우저에서 복사하여 파일로 저장하고 해당 파일을 unit test로 작성하면 차단될 가능도 적어집니다.

[![image 12](C:\GitHub\computer_note\Language\Python\Crawling\images\image_12.png)](https://blog.advenoh.pe.kr/static/62dbc289e5c3bf1f0387400e2128b5e8/587b0/image_12.png)

저장된 HTML를 불러오는 코드 부분입니다. 전체 코드는 github에 업로드된 파일을 참조해주세요.

```python
import unittest
import web_scraping as ws

class WebScrapingTest(unittest.TestCase):
FILE_URL = “main_news.html” chrome에서 복사해서

def test_(self):
ws.parse_and_process(open(self.FILE_URL))

if __name__ == '__main__':
unittest.main()
```

# 4. 참고

이 포스트에서 작성된 파일은 [깃허브](https://github.com/kenshin579/tutorials-python/tree/master/web_scraping)에서 확인할 수 있습니다.

- 관련 책
  - [Python Web Scraping](https://www.amazon.com/Scraping-Python-Community-Experience-Distilled/dp/1782164367)![image 9](C:\GitHub\computer_note\Language\Python\Crawling\images\image_9.jpg)
  - [Web Scraping with Python](http://www.hanbit.co.kr/store/books/look.php?p_code=B7159663510)![image 10](C:\GitHub\computer_note\Language\Python\Crawling\images\image_10.jpg)
- Robots.txt
  - http://www.robotstxt.org/robotstxt.html
  - https://www.twinword.co.kr/blog/basic-technical-seo/
  - http://www.searchengines.co.kr/mkt-bootcamp/7807
- 웹 스크래핑 차단 방지
  - https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/
  - https://hackernoon.com/web-scraping-tutorial-with-python-tips-and-tricks-db070e70e071
- Tor
  - [https://namu.wiki/w/Tor(%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4)](https://namu.wiki/w/Tor(소프트웨어))
  - https://kremalicious.com/simple-tor-setup-on-mac-os-x/
  - http://act.jinbo.net/wp/4449/
  - https://gist.github.com/DusanMadar/8d11026b7ce0bce6a67f7dd87b999f6b
  - Tor 윈도우 설치
    - https://guide.jinbo.net/digital-security/communication-security/how-to-use-tor

